diff -u -- 06_1801111587/q1/common.py denoising/common.py
--- 06_1801111587/q1/common.py	2019-06-13 15:13:25.627019100 +0800
+++ denoising/common.py	2019-06-01 05:10:06.000000000 +0800
@@ -22,7 +22,7 @@
     show_interval = 2
     snapshot_interval = 2
     test_interval = 1
-    sigma = 15
+    sigma = 50
     use_extra_data = True
     scale = [1, 0.9, 0.8, 0.7]
     @property
diff -u -- 06_1801111587/q1/dataset.py denoising/dataset.py
--- 06_1801111587/q1/dataset.py	2019-06-13 15:13:25.597093100 +0800
+++ denoising/dataset.py	2019-06-01 05:10:06.000000000 +0800
@@ -10,11 +10,11 @@
 
 class Dataset():
 
-    def __init__(self, dataset_name, noise_level=15):
+    def __init__(self, dataset_name, noise_level=50):
 
-        test_dataset_path = '../dataset/CBSD68'
-        test_noisy_path = '../dataset/CBSD68_{}'.format(noise_level)
-        train_dataset_path = '../dataset/CBSD432/CBSD432'
+        test_dataset_path = '../../dataset/CBSD68/CBSD68'
+        test_noisy_path = '../../dataset/CBSD68/CBSD68_{}'.format(noise_level)
+        train_dataset_path = '../../dataset/CBSD432/CBSD432'
         self.minibatch_size = config.minibatch_size
         self.ds_name = dataset_name
         self.rng = np.random
@@ -86,7 +86,7 @@
             imggrid.append(noisy_patch)
         imggrid = np.array(imggrid).reshape((4, 4, img.shape[0], img.shape[1], img.shape[2]))
         imggrid = imggrid.transpose((0, 2, 1, 3, 4)).reshape((4*img.shape[0], 4*img.shape[1], 3))
-        cv2.imshow('imshow', imggrid)
+        cv2.imshow('', imggrid)
         c = chr(cv2.waitKey(0) & 0xff)
         if c == 'q':
             exit()
diff -u -- 06_1801111587/q1/model.py denoising/model.py
--- 06_1801111587/q1/model.py	2019-06-12 00:32:55.209894300 +0800
+++ denoising/model.py	2019-06-01 05:10:06.000000000 +0800
@@ -5,6 +5,7 @@
 import tensorflow.contrib as tf_contrib
 from common import config
 
+
 class Model():
     def __init__(self, depth = 17):
         # set the initializer of conv_weight and conv_bias
@@ -12,7 +13,7 @@
                                 mode='FAN_IN', uniform=False)
         self.bias_init = tf.zeros_initializer()
         self.depth = depth
-        self.reg = tf_contrib.layers.l2_regular izer(config.weight_decay)
+        self.reg = tf_contrib.layers.l2_regularizer(config.weight_decay)
 
     def _conv_layer(self, name, inp, kernel_shape, stride, padding='SAME',is_training=False, include_bn = True, include_relu = True ):
         with tf.variable_scope(name) as scope:
@@ -49,7 +50,7 @@
         x = self._conv_layer(name='conv' + str(self.depth), inp = x, kernel_shape=[3, 3, 64, config.nr_channel],
                              stride=1, is_training=is_training, include_bn = False, include_relu = False)
 
-        output = data - x ## residual learning,x是残差，噪声图片减去残差就是真实的
+        output = data - x ## residual learning
 
         placeholders = {
             'data': data,
diff -u -- 06_1801111587/q1/prepare_data.py denoising/prepare_data.py
--- 06_1801111587/q1/prepare_data.py	2019-06-10 12:30:11.612722000 +0800
+++ denoising/prepare_data.py	2019-06-01 05:10:06.000000000 +0800
@@ -4,12 +4,12 @@
 import glob
 import argparse
 
-test_dataset_path = '../dataset/CBSD68'
+test_dataset_path = '../CBSD68/CBSD68'
 test_list = glob.glob(test_dataset_path + '/*.png')
 dataset_meta = (test_list, len(test_list))
 
 def main(args):
-    target_path = os.path.join('../dataset','CBSD68_{}'.format(args.noise_level))
+    target_path = os.path.join('../CBSD68','CBSD68_{}'.format(args.noise_level))
     if not os.path.exists(target_path):
         os.system('mkdir {}'.format(target_path))
     for i in range(len(test_list)):
@@ -21,7 +21,7 @@
 
 if __name__ == '__main__':
     parser = argparse.ArgumentParser()
-    parser.add_argument('--noise_level',type = int,default=15)
+    parser.add_argument('--noise_level',type = int)
     args = parser.parse_args()
     main(args)
 
diff -u -- 06_1801111587/q1/train.py denoising/train.py
--- 06_1801111587/q1/train.py	2019-06-13 15:13:25.568007900 +0800
+++ denoising/train.py	2019-06-01 05:10:06.000000000 +0800
@@ -5,7 +5,6 @@
 import argparse
 import tensorflow as tf
 import numpy as np
-import cv2
 
 from model import Model
 from dataset import Dataset
@@ -13,7 +12,7 @@
 from common import config
 
 
-def get_dataset_batch(ds_name, noise_level = 15):
+def get_dataset_batch(ds_name, noise_level = 50):
     dataset = Dataset(ds_name, noise_level)
     ds_gnr = dataset.load().instance_generator
     ds = tf.data.Dataset.from_generator(ds_gnr, output_types=(tf.float32, tf.float32),)
@@ -40,7 +39,7 @@
     ## load dataset
     train_batch_gnr, train_set = get_dataset_batch(ds_name='train')
 
-    test_gnr, test_set = get_dataset_batch(ds_name = 'test', noise_level = 15)
+    test_gnr, test_set = get_dataset_batch(ds_name = 'test', noise_level = 50)
     ## build graph
     network = Model()
     placeholders, restored = network.build()
@@ -84,9 +83,8 @@
             epoch_start = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[1])
             global_cnt = epoch_start * train_set.minibatchs_per_epoch
 
-        file=open('./psnr_sigma_15','w+')
         ## training
-        for epoch in range(epoch_start + 1, config.nr_epoch + 1):
+        for epoch in range(epoch_start+1, config.nr_epoch+1):
             for _ in range(train_set.minibatchs_per_epoch):
                 global_cnt += 1
                 images, noisy_images = sess.run(train_batch_gnr)
@@ -98,7 +96,7 @@
                 }
                 _, loss_v, loss_reg_v, lr_v, summary = sess.run([train, loss, loss_reg,
                                                                  lr, merged],
-                                                                feed_dict=feed_dict)
+                                                                       feed_dict=feed_dict)
                 if global_cnt % config.show_interval == 0:
                     train_writer.add_summary(summary, global_cnt)
                     print(
@@ -109,7 +107,7 @@
                         'lr: {:.4f}'.format(lr_v),
                     )
 
-                ## save model
+            ## save model
             if epoch % config.snapshot_interval == 0:
                 saver.save(sess, os.path.join(config.log_model_dir, 'epoch-{}'.format(epoch)),
                            global_step=global_cnt)
@@ -122,17 +120,11 @@
                         placeholders['data']: noisy_image,
                         placeholders['is_training']: False,
                     }
-                    restored_v = sess.run([restored], feed_dict=feed_dict)
-                    if epoch==159:
-                        img = np.clip(restored_v[0][0, :, :, :], 0, 1) * 255
-                        img = img.astype('uint8')
-                        cv2.imwrite('./output/{}.png'.format(global_cnt), img)
-                    psnr_x = compare_psnr(image[0, :, :, ::-1], restored_v[0][0, :, :, ::-1])
+                    restored_v = sess.run([restored],feed_dict = feed_dict)
+                    psnr_x = compare_psnr(image[0,:,:,::-1], restored_v[0][0, :, :, ::-1])
                     psnrs.append(psnr_x)
                 print('average psnr is {:2.2f} dB'.format(np.mean(psnrs)))
-                file.write(str(np.mean(psnrs)) + '\n')
-            print('Training is done, exit.')
-        file.close()
+        print('Training is done, exit.')
 
 if __name__ == "__main__":
     try:
Only in 06_1801111587/q1: train1.py.baiduyun.uploading.cfg
